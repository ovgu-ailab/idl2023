---
layout: default
title: Reading Assignment
id: reading9
---


# Reading Assignment: Autoencoders and Self-Supervised Representation Learning

This reading assignemt focusses on unsupervised and self-supervised learning tasks as a main driver for representation learning.

## Main Reading 1: Autoencoders

* The [Deep Learning Book - Chapter 14: Autoencoders](http://www.deeplearningbook.org/contents/autoencoders.html) (optional reading!) covers the topic very well in depth.

* However, in favor of including the second topic, the [blog post "Introduction to autoencoders" by Jeremy Jordan](https://www.jeremyjordan.me/autoencoders/) provides the most important details.

Refer to the [book chapter](http://www.deeplearningbook.org/contents/autoencoders.html) if you would like to know further details!

## Main Reading 2: Self-Supervised Representation Learning

* Dive deeper into self-supervised representation learning approaches (like BERT) with [Lilian Weng's overview blog post "Self-Supervised Representation Learning"](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html).

* Continue with the follow-up post ["Contrastive Representation Learning"](https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html) that covers very recent developments.

* Optionally, the [blog post "Unsupervised Cross-lingual Representation Learning" by Sebastian Ruder](https://ruder.io/unsupervised-cross-lingual-learning/) provides a glimpse at an exciting new NLP task that is tackled by deep learning.

* Further optionally, the [Deep Learning Book - Chapter 15: Representation Learning](https://www.deeplearningbook.org/contents/representation.html) also addresses the topic but does not cover the most recent work (sincs 2015).

## Further Reading (Optional)

* For further in-depth reading, follow the links from the blog posts to the referenced articles.

